{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Anggota Kelompok (Kelas N (Prodi RKA)):\n",
        "\n",
        "- Chelsea (5054241010)\n",
        "- Wayan Raditya Putra (5054241029)\n",
        "- Syauqi Nabil Tasri (5054241040)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rVC7Sea87PQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dependencies Installing**"
      ],
      "metadata": {
        "id": "OnwK3mAFW4Oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sebelum memulai projek, alangkah baiknya jika kita menyiapkan dan menginstall tools - tools yang diperlukan."
      ],
      "metadata": {
        "id": "1zdfjHHYXNle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install requests beautifulsoup4 lxml dateparser"
      ],
      "metadata": {
        "id": "aa6yEr5i9vU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**lxml** adalah pustaka pengurai **XML** dan **HTML** berkinerja tinggi untuk Python, yang dikenal karena kecepatan dan rangkaian fiturnya yang lengkap.\n",
        "\n",
        "Library ini menyediakan cara yang cepat dan efisien untuk mengurai, memanipulasi, dan mengekstrak data dari berkas **XML** dan **HTML** menggunakan **API** mirip ElementTree yang dipadukan dengan kecepatan pustaka libxml2 dan libxslt. lxml banyak digunakan dalam pengikisan web, ekstraksi data, dan tugas-tugas lain yang memerlukan penanganan data terstruktur dari sumber **XML** atau **HTML**."
      ],
      "metadata": {
        "id": "CTGwTixT-sOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Library**"
      ],
      "metadata": {
        "id": "1yeyN8iwWaHX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYaSxDIdw8QE"
      },
      "outputs": [],
      "source": [
        "import re, json, time, random\n",
        "from typing import List, Dict, Any, Optional\n",
        "from urllib.parse import urlparse, urljoin, urlencode, parse_qs\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "import urllib.robotparser as robotparser\n",
        "import dateparser\n",
        "from datetime import datetime, date\n",
        "import difflib\n",
        "from collections import defaultdict\n",
        "from urllib.parse import urlparse, urlunparse\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan\n",
        "\n",
        "1. re, json, time, random: Untuk ekspresi reguler, pengolahan JSON, pengaturan waktu, dan fungsi acak.\n",
        "2. typing: Memberikan petunjuk tipe data agar kode lebih jelas.\n",
        "3. urllib.parse: Untuk memparsing dan memanipulasi URL.\n",
        "4. requests: Untuk mengirim permintaan HTTP.\n",
        "5. BeautifulSoup: Untuk memparsing dan mengekstrak data dari HTML.\n",
        "6. HTTPAdapter, Retry: Untuk mengatur pengulangan permintaan HTTP agar lebih handal.\n",
        "7. robotparser: Untuk memeriksa aturan robots.txt pada situs web agar crawler mematuhi batasan.\n",
        "8. dateparser, datetime: Untuk memparsing dan mengelola tanggal dan waktu.\n",
        "9. difflib: Untuk membandingkan urutan data, berguna untuk pencocokan yang mirip.\n",
        "10. defaultdict: Tipe dictionary yang memudahkan pengelolaan nilai default.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rQSad8HUy1Mr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuration Setting**"
      ],
      "metadata": {
        "id": "kEbOy-dSWeCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disini kita perlu menyesuaikan beberapa konfigurasi yang dibutuhkan nanti sepert url, max page yang dibaca, timeout, veriabel untuk filtering (keyword, start date, end date), dan yang lainnya."
      ],
      "metadata": {
        "id": "XL5k8SUDXt_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SECTION_URL = \"https://nasional.kompas.com/\"\n",
        "\n",
        "need_date_from_detail = True\n",
        "RESPECT_ROBOTS = True\n",
        "HARD_CAP       = 10\n",
        "DELAY_S        = (0.7, 1.6)\n",
        "TIMEOUT_S      = 20\n",
        "UA             = \"TestScraper/1.0\"\n",
        "LANG_HDR       = \"id,en;q=0.9\"\n",
        "\n",
        "KEYWORDS = [\"sahroni\"]\n",
        "START_DATE = \"2025-01-09\"\n",
        "END_DATE   = \"2025-04-09\"\n",
        "\n",
        "MATCH_IN = \"title\"\n",
        "CASE_SENSITIVE = False\n",
        "INCLUDE_WITHOUT_DATE = False"
      ],
      "metadata": {
        "id": "nmgMfgOa89i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dalam configure setting ini kita perlu menyesuaikan beberapa konfigurasi yang dibutuhkan nanti sepert url, max page yang dibaca, timeout, veriabel untuk filtering (keyword, start date, end date), dan yang lainnya.\n",
        "\n",
        "nah maka dari itu kode ini dipakai dalam mengatur parameter untuk scraper berita yang akan mengambil maks 10 artikel dari halaman nasional kompas pada tanggal 9 Jan 2025, mencari kata kunci tertentu di judul artikel, dengan pengaturan agar scraper dapat memahami aturan robots.txt, menggunakan delay acak antar permintaan, dan mengambil tanggal berita dari halaman detail jika diperlukan"
      ],
      "metadata": {
        "id": "JzpoXTXV5KWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Session HTTP**\n",
        "\n",
        "Dalam konteks session **HTTP**, fitur dari library requests di Python, di mana objek Session digunakan untuk mengelola dan mempertahankan koneksi **HTTP** secara efisien selama beberapa permintaan. Kode ini mengatur sesi **HTTP** yang handal dengan mekanisme retry dan header khusus menggunakan objek **requests.Session()** untuk menjaga koneksi tetap efisien dan konsisten.\n",
        "Selain itu, kode ini juga memeriksa aturan **robots.txt** sebelum mengakses halaman, mengambil dan memparsing halaman **HTML** menjadi objek BeautifulSoup, serta mengonversi string tanggal dalam bahasa Indonesia atau Inggris menjadi objek tanggal Python yang dapat digunakan untuk filter atau analisis data.\n",
        "Semua ini merupakan bagian penting dalam membuat scraper web yang efisien, sopan, dan akurat."
      ],
      "metadata": {
        "id": "cWgGXS5BWq19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session = requests.Session()\n",
        "retries = Retry(total=4, backoff_factor=0.5,\n",
        "                status_forcelist=[429, 500, 502, 503, 504],\n",
        "                allowed_methods=[\"GET\"])\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\n",
        "    \"User-Agent\": UA,\n",
        "    \"Accept-Language\": LANG_HDR,\n",
        "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "})\n",
        "\n",
        "def is_allowed_by_robots(url: str, user_agent: str = UA) -> bool:\n",
        "    \"\"\"Cek robots.txt; kalau gagal baca → anggap tidak boleh (konservatif).\"\"\"\n",
        "    try:\n",
        "        p = urlparse(url)\n",
        "        robots_url = f\"{p.scheme}://{p.netloc}/robots.txt\"\n",
        "        rp = robotparser.RobotFileParser()\n",
        "        rp.set_url(robots_url); rp.read()\n",
        "        return bool(rp.can_fetch(user_agent, url))\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def get_soup(url: str) -> BeautifulSoup:\n",
        "    \"\"\"HTTP GET + parse HTML ke BeautifulSoup (parser lxml).\"\"\"\n",
        "    r = session.get(url, timeout=TIMEOUT_S)\n",
        "    r.raise_for_status()\n",
        "    return BeautifulSoup(r.text, \"lxml\")\n",
        "\n",
        "def parse_id_date(s: Optional[str]):\n",
        "    \"\"\"Parse string tanggal (ID/EN) → datetime.date, atau None.\"\"\"\n",
        "    if not s: return None\n",
        "    dt = dateparser.parse(\n",
        "        s,\n",
        "        languages=[\"id\", \"en\"],\n",
        "        settings={\"DATE_ORDER\": \"DMY\", \"TIMEZONE\": \"Asia/Jakarta\", \"RETURN_AS_TIMEZONE_AWARE\": False}\n",
        "    )\n",
        "    return dt.date() if dt else None"
      ],
      "metadata": {
        "id": "EI3q3XqoDlW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extractor Function\n",
        "\n",
        "def _jsonld_blocks(soup: BeautifulSoup) -> List[dict]:\n",
        "    blocks = []\n",
        "    for sc in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
        "        raw = sc.string or \"\"\n",
        "        try:\n",
        "            data = json.loads(raw)\n",
        "        except Exception:\n",
        "            continue\n",
        "        if isinstance(data, list):\n",
        "            blocks.extend([x for x in data if isinstance(x, dict)])\n",
        "        elif isinstance(data, dict):\n",
        "            blocks.append(data)\n",
        "    return blocks\n",
        "\n",
        "def _is_article_url(href: str, base_url: str) -> bool:\n",
        "    \"\"\"Validasi URL artikel (domain sama + pola umum).\"\"\"\n",
        "    if not href: return False\n",
        "    abs_url = urljoin(base_url, href)\n",
        "    u = urlparse(abs_url)\n",
        "    if u.netloc != urlparse(base_url).netloc: return False\n",
        "    q = (u.query or \"\").lower()\n",
        "    if \"utm_\" in q or \"source=navbar\" in q: return False\n",
        "    # Pola umum Kompas/KompasTV untuk artikel:\n",
        "    return bool(re.search(r\"/read/|/berita/|/video/\", u.path))\n",
        "\n",
        "def extract_from_itemlist(soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Ambil artikel dari JSON-LD ItemList/Collection/SearchResults.\"\"\"\n",
        "    rows = []\n",
        "    for b in _jsonld_blocks(soup):\n",
        "        typ = b.get(\"@type\")\n",
        "        if isinstance(typ, list): typ = typ[0] if typ else None\n",
        "        if str(typ).lower() in (\"itemlist\",\"collectionpage\",\"searchresultspage\"):\n",
        "            for el in b.get(\"itemListElement\", []):\n",
        "                node = el.get(\"item\", el)\n",
        "                if not isinstance(node, dict): continue\n",
        "                url = node.get(\"url\") or node.get(\"@id\")\n",
        "                name = node.get(\"name\") or node.get(\"headline\") or node.get(\"title\")\n",
        "                dp   = node.get(\"datePublished\") or node.get(\"dateCreated\")\n",
        "                if url and _is_article_url(url, base_url):\n",
        "                    rows.append({\n",
        "                        \"title\": (name or \"\").strip(),\n",
        "                        \"url\": urljoin(base_url, url),\n",
        "                        \"published_raw\": dp,\n",
        "                        \"summary\": node.get(\"description\"),\n",
        "                    })\n",
        "    return list({r[\"url\"]: r for r in rows}.values())\n",
        "\n",
        "def extract_from_dom(soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Fallback DOM: cari anchor artikel di area <main>.\"\"\"\n",
        "    rows = []\n",
        "    main = soup.select_one(\"main\") or soup\n",
        "    for a in main.select(\"a[href]\"):\n",
        "        href = a.get(\"href\")\n",
        "        if not _is_article_url(href, base_url):\n",
        "            continue\n",
        "        title = a.get_text(\" \", strip=True)\n",
        "        if not title:\n",
        "            continue\n",
        "        url = urljoin(base_url, href)\n",
        "        # Timestamp di sekitar kartu\n",
        "        ts = None\n",
        "        card = a.find_parent([\"article\",\"div\",\"li\"])\n",
        "        if card:\n",
        "            t = card.find(\"time\")\n",
        "            if t:\n",
        "                ts = t.get(\"datetime\") or t.get_text(\" \", strip=True)\n",
        "            else:\n",
        "                # KompasTV kadang pakai span teks tanggal (mis. \"1 September 2025\")\n",
        "                near = card.find(lambda tag: tag.name in (\"span\",\"div\",\"p\")\n",
        "                                 and re.search(r\"\\b\\d{1,2}\\s\\w+\\s\\d{4}\\b|\\bWIB\\b\", tag.get_text(\" \", strip=True)))\n",
        "                if near:\n",
        "                    ts = near.get_text(\" \", strip=True)\n",
        "        rows.append({\"title\": title, \"url\": url, \"published_raw\": ts})\n",
        "    return list({r[\"url\"]: r for r in rows}.values())\n",
        "\n",
        "def parse_article_detail(url: str) -> Dict[str, Any]:\n",
        "    \"\"\"Ambil judul/tanggal/body dari halaman artikel (selector → meta → JSON-LD → URL path).\"\"\"\n",
        "    s = get_soup(url)\n",
        "\n",
        "    # Bersihkan noise\n",
        "    for junk in s.select(\"script, style, .ads, .advertisement, .share, .social-share\"):\n",
        "        junk.decompose()\n",
        "\n",
        "    # Judul\n",
        "    title_el = s.select_one(\"h1\") or s.select_one(\"h1.article__title\")\n",
        "    title = title_el.get_text(\" \", strip=True) if title_el else None\n",
        "\n",
        "    # Tanggal (urutan prioritas)\n",
        "    published_raw = None\n",
        "    t = s.select_one(\"time\")\n",
        "    if t:\n",
        "        published_raw = t.get(\"datetime\") or t.get_text(\" \", strip=True)\n",
        "\n",
        "    if not published_raw:  # meta tag umum\n",
        "        for css in [\n",
        "            'meta[itemprop=\"datePublished\"]',\n",
        "            'meta[property=\"article:published_time\"]',\n",
        "            'meta[name=\"date\"]',\n",
        "            'meta[name=\"publishdate\"]',\n",
        "            'meta[name=\"pubdate\"]',\n",
        "            'meta[property=\"og:published_time\"]',\n",
        "            'meta[property=\"article:modified_time\"]',\n",
        "        ]:\n",
        "            m = s.select_one(css)\n",
        "            if m and m.has_attr(\"content\") and m[\"content\"]:\n",
        "                published_raw = m[\"content\"].strip()\n",
        "                break\n",
        "\n",
        "    if not published_raw or not title:  # JSON-LD NewsArticle\n",
        "        for b in _jsonld_blocks(s):\n",
        "            typ = b.get(\"@type\")\n",
        "            if isinstance(typ, list): typ = typ[0] if typ else None\n",
        "            if str(typ).lower() in (\"newsarticle\",\"article\",\"blogposting\"):\n",
        "                title = title or b.get(\"headline\") or b.get(\"name\") or b.get(\"title\")\n",
        "                published_raw = published_raw or b.get(\"datePublished\") or b.get(\"dateCreated\")\n",
        "            if title and published_raw:\n",
        "                break\n",
        "\n",
        "    if not published_raw:  # fallback URL path /YYYY/MM/DD/\n",
        "        m = re.search(r\"/(\\d{4})/(\\d{2})/(\\d{2})/\", url)\n",
        "        if m:\n",
        "            published_raw = f\"{m.group(1)}-{m.group(2)}-{m.group(3)}\"\n",
        "\n",
        "    # Body (opsional)\n",
        "    body = None\n",
        "    for sel in [\"article .read__content\", \"article .article__body\", \".article__content\", \"article\"]:\n",
        "        el = s.select_one(sel)\n",
        "        if el:\n",
        "            body = el.get_text(\" \", strip=True)\n",
        "            if body: break\n",
        "\n",
        "    return {\"title\": title, \"published_raw\": published_raw, \"text\": body}"
      ],
      "metadata": {
        "id": "Vj4abKIJ-QJY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extractor Function (mengambil (mengekstrak) informasi artikel dari halaman web)**\n",
        "\n",
        "1. `_jsonld_blocks(soup)`  \n",
        "Mengambil semua blok **JSON-LD** dari halaman HTML tag `<script type=\"application/ld+json\">`,  \n",
        "lalu mengembalikan daftar objek JSON yang valid.  \n",
        "**Tujuan:** ambil metadata artikel yang sering disimpan dalam format JSON-LD.  \n",
        "\n",
        "2. `_is_article_url(href, base_url)`  \n",
        "Validasi apakah sebuah URL benar-benar link artikel, dengan syarat:  \n",
        "- Masih dalam domain yang sama  \n",
        "- Bukan link tracking (`utm_`, `source=navbar`)  \n",
        "- Path mengandung `/read/`, `/berita/`, atau `/video/`  \n",
        "**Tujuan:** filter agar hanya dapat URL artikel asli.  \n",
        "\n",
        "3. `extract_from_itemlist(soup, base_url)`  \n",
        "Mengekstrak daftar artikel dari blok JSON-LD bertipe **ItemList**, **CollectionPage**, atau **SearchResultsPage**.  \n",
        "Mengambil:\n",
        "- Judul  \n",
        "- URL  \n",
        "- Tanggal publikasi  \n",
        "- Ringkasan artikel  \n",
        "\n",
        "4. `extract_from_dom(soup, base_url)`  \n",
        "*fallback* jika JSON-LD tidak ada.  \n",
        "Mencari link artikel langsung di dalam elemen `<main>` pada HTML:  \n",
        "- Cek anchor `<a>` yang memenuhi kriteria URL artikel  \n",
        "- Ambil teks link sebagai judul  \n",
        "- Coba cari tanggal di tag `<time>` atau teks tanggal dalam `<span>/<div>`  \n",
        "**Tujuan:** alternatif scraping langsung dari HTML DOM.  \n",
        "\n",
        "5. `parse_article_detail(url)`  \n",
        "Mengambil detail isi artikel dari halaman tertentu:  \n",
        "- **Judul** → dari `<h1>` atau JSON-LD  \n",
        "- **Tanggal** → dari `<time>`, meta tag umum, JSON-LD, atau fallback ke URL `/YYYY/MM/DD/`  \n",
        "- **Isi teks** → dari `article .read__content`, `.article__body`, `.article__content`, atau `article`  \n",
        "**Hasil:** `{title, published_raw, text}`  \n"
      ],
      "metadata": {
        "id": "OmpVzNf-j4lU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pagination Function\n",
        "\n",
        "def _replace_page_param(url: str, page_num: int) -> str:\n",
        "    \"\"\"Set/ubah ?page=N pada URL (menjaga path & query lain).\"\"\"\n",
        "    u = urlparse(url)\n",
        "    qs = parse_qs(u.query)\n",
        "    qs[\"page\"] = [str(page_num)]\n",
        "    new_q = urlencode({k: (v[0] if len(v)==1 else v) for k,v in qs.items()}, doseq=True)\n",
        "    return u._replace(query=new_q).geturl()\n",
        "\n",
        "def _extract_page_numbers_from_dom(soup: BeautifulSoup) -> List[int]:\n",
        "    \"\"\"Kumpulkan kandidat nomor halaman dari anchor pagination.\"\"\"\n",
        "    nums = set()\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        t = (a.get_text(strip=True) or \"\")\n",
        "        if t.isdigit():\n",
        "            nums.add(int(t))\n",
        "        m = re.search(r\"[?&]page=(\\d+)\", a[\"href\"])\n",
        "        if m:\n",
        "            nums.add(int(m.group(1)))\n",
        "        if re.search(r\"(Terakhir|Last|>>|»)\", t, re.I):\n",
        "            m2 = re.search(r\"[?&]page=(\\d+)\", a[\"href\"])\n",
        "            if m2:\n",
        "                nums.add(int(m2.group(1)))\n",
        "    return sorted(n for n in nums if n > 0)\n",
        "\n",
        "def _find_next_by_dom(soup: BeautifulSoup, current_url: str) -> Optional[str]:\n",
        "    \"\"\"Cari tautan halaman berikutnya lewat DOM (rel=next / teks Next/›/»/Selanjutnya/Berikutnya).\"\"\"\n",
        "    a = soup.select_one('a[rel=\"next\"]')\n",
        "    if a and a.has_attr(\"href\"):\n",
        "        return urljoin(current_url, a[\"href\"])\n",
        "    a = soup.find(\"a\", string=re.compile(r\"(Selanjutnya|Berikutnya|Next|›|»)\", re.I))\n",
        "    if a and a.has_attr(\"href\"):\n",
        "        return urljoin(current_url, a[\"href\"])\n",
        "    # angka current+1\n",
        "    cur = int(parse_qs(urlparse(current_url).query).get(\"page\", [\"1\"])[0] or 1)\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        t = (a.get_text(strip=True) or \"\")\n",
        "        if t.isdigit() and int(t) == cur + 1:\n",
        "            return urljoin(current_url, a[\"href\"])\n",
        "    return None\n",
        "\n",
        "def collect_list_page(url: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Ambil semua artikel dari 1 halaman list (Kompas/KompasTV).\"\"\"\n",
        "    base = f\"{urlparse(url).scheme}://{urlparse(url).netloc}\"\n",
        "    soup = get_soup(url)\n",
        "    rows = []\n",
        "    rows += extract_from_itemlist(soup, base)  # sering ada di Kompas.com; kadang ada di KompasTV\n",
        "    rows += extract_from_dom(soup, base)       # fallback andalan (KompasTV)\n",
        "    # parse tanggal awal + de-dup\n",
        "    uniq = {}\n",
        "    for r in rows:\n",
        "        r[\"published\"] = parse_id_date(r.get(\"published_raw\"))\n",
        "        uniq[r[\"url\"]] = r\n",
        "    return list(uniq.values()), soup\n",
        "\n",
        "def crawl_section_all_pages(section_url: str,\n",
        "                            need_date_from_detail: bool = True,\n",
        "                            hard_cap: int = HARD_CAP) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Jelajah semua halaman:\n",
        "    - Ikuti 'next' via DOM; jika tidak ada, fallback ke ?page=N (2,3,...) sampai kosong.\n",
        "    - Berhenti saat tidak ada halaman berikutnya atau mencapai hard_cap.\n",
        "    \"\"\"\n",
        "    if RESPECT_ROBOTS and not is_allowed_by_robots(section_url):\n",
        "        print(\"[robots] Disallowed:\", section_url)\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "    seen_urls = set()\n",
        "    url = section_url\n",
        "    page = 1\n",
        "\n",
        "    while url and page <= hard_cap and url not in seen_urls:\n",
        "        seen_urls.add(url)\n",
        "        print(f\"[crawl] page {page}: {url}\")\n",
        "\n",
        "        rows, soup = collect_list_page(url)\n",
        "        if not rows and page > 1:\n",
        "            print(\"[crawl] no rows → stop\")\n",
        "            break\n",
        "\n",
        "        results.extend(rows)\n",
        "\n",
        "        # 1) coba next dari DOM\n",
        "        nxt = _find_next_by_dom(soup, url)\n",
        "\n",
        "        # 2) kalau tidak ada, pakai fallback ?page=N\n",
        "        if not nxt:\n",
        "            nxt = _replace_page_param(url, page + 1)\n",
        "\n",
        "        # guard: kalau next sama dgn current, berhenti\n",
        "        if nxt == url:\n",
        "            break\n",
        "\n",
        "        url = nxt\n",
        "        page += 1\n",
        "        time.sleep(random.uniform(*DELAY_S))\n",
        "\n",
        "    # De-dup global\n",
        "    results = list({r[\"url\"]: r for r in results}.values())\n",
        "\n",
        "    # Lengkapi tanggal dari detail bila perlu\n",
        "    if need_date_from_detail:\n",
        "        for r in results:\n",
        "            if r.get(\"published\") is None:\n",
        "                try:\n",
        "                    d = parse_article_detail(r[\"url\"])\n",
        "                    r[\"title\"] = r.get(\"title\") or d.get(\"title\")\n",
        "                    if not r.get(\"published\"):\n",
        "                        r[\"published\"] = parse_id_date(d.get(\"published_raw\"))\n",
        "                except Exception as e:\n",
        "                    r[\"error_detail\"] = str(e)\n",
        "                time.sleep(random.uniform(*DELAY_S))\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "3NK3Uui5-4MM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_replace_page_param(url, page_num)**\n",
        "\n",
        "Mengubah atau menambahkan parameter query page=N pada URL tanpa mengubah bagian path atau query lain.\n",
        "\n",
        "**_extract_page_numbers_from_dom(soup)**\n",
        "\n",
        "Mengumpulkan nomor halaman yang tersedia dari tautan pagination di halaman (misal angka halaman atau parameter page di URL).\n",
        "\n",
        "**_find_next_by_dom(soup, current_url)**\n",
        "\n",
        "Mencari tautan halaman berikutnya dengan memeriksa:\n",
        "\n",
        "1. anchor dengan atribut rel=\"next\",\n",
        "2. anchor dengan teks seperti \"Next\", \"Selanjutnya\", \"›\", \"»\",\n",
        "3. anchor dengan nomor halaman yang satu angka lebih besar dari halaman saat ini.\n",
        "\n",
        "**collect_list_page(url)**\n",
        "\n",
        "Mengambil semua artikel dari satu halaman daftar artikel dengan memanggil fungsi ekstraksi JSON-LD dan DOM, lalu mengurai tanggal publikasi dan menghilangkan duplikat.\n",
        "\n",
        "**crawl_section_all_pages(section_url, need_date_from_detail=True, hard_cap=HARD_CAP)**\n",
        "\n",
        "Fungsi utama untuk menjelajah (crawl) semua halaman daftar artikel pada sebuah section:\n",
        "\n",
        "1. Mengikuti tautan halaman berikutnya yang ditemukan lewat DOM (rel=next atau teks next).\n",
        "2. Jika tidak ada tautan next, menggunakan fallback dengan menambah parameter ?page=N.\n",
        "3. Berhenti jika tidak ada halaman berikutnya, sudah mencapai batas maksimal halaman (hard_cap), atau URL halaman berikutnya sama dengan halaman saat ini.\n",
        "4. Mengumpulkan semua artikel dari tiap halaman, menghilangkan duplikat, dan jika perlu melengkapi tanggal publikasi dengan mengambil detail artikel satu per satu."
      ],
      "metadata": {
        "id": "LjPBg3hrqidt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Eksekusi Crawler pada Satu Section**\n",
        "\n",
        "Setelah semua fungsi pendukung dibuat, tahap berikutnya adalah menjalankan crawler utama.  \n",
        "Di sini kita memanggil fungsi `crawl_section_all_pages` dengan parameter:  \n",
        "\n",
        "- **SECTION_URL** → alamat awal section artikel yang ingin diambil.  \n",
        "- **need_date_from_detail=True** → jika tanggal publikasi belum lengkap, crawler akan membuka halaman detail artikel.  \n",
        "- **hard_cap=HARD_CAP** → batas maksimal jumlah halaman yang boleh dijelajahi, agar crawler tidak berjalan tanpa henti.  \n",
        "\n",
        "Hasil eksekusi disimpan dalam variabel `rows`, yaitu daftar semua artikel yang berhasil dikumpulkan.  \n",
        "Kemudian, kita cetak jumlah total item yang terkumpul beserta section URL-nya.  "
      ],
      "metadata": {
        "id": "Bn5Lte9GgKT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = crawl_section_all_pages(SECTION_URL, need_date_from_detail=True, hard_cap=HARD_CAP)\n",
        "print(f\"\\nTotal items collected: {len(rows)}  (section: {SECTION_URL})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iyi0lScVzc3",
        "outputId": "e9d99555-a8ce-48fd-b47c-59a9f0575fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[crawl] page 1: https://nasional.kompas.com/\n",
            "[crawl] page 2: https://nasional.kompas.com/?page=2\n",
            "[crawl] page 3: https://nasional.kompas.com/?page=3\n",
            "[crawl] page 4: https://nasional.kompas.com/?page=4\n",
            "[crawl] page 5: https://nasional.kompas.com/?page=5\n",
            "[crawl] page 6: https://nasional.kompas.com/?page=6\n",
            "[crawl] page 7: https://nasional.kompas.com/?page=7\n",
            "[crawl] page 8: https://nasional.kompas.com/?page=8\n",
            "[crawl] page 9: https://nasional.kompas.com/?page=9\n",
            "[crawl] page 10: https://nasional.kompas.com/?page=10\n",
            "\n",
            "Total items collected: 200  (section: https://nasional.kompas.com/)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Penjelasan Fungsi - Data Duplicate Detection and Remover**\n",
        "\n",
        "**1. canonicalize_url(u: str)**  \n",
        "\n",
        "Membuat versi standar dari sebuah URL (kanonik), supaya variasi kecil dianggap sama.  \n",
        "- Menghapus query string seperti ?utm atau ?source, dan juga fragment (#).  \n",
        "- Menghilangkan suffix /amp di akhir path.  \n",
        "- Merapikan tanda slash berlebih.  \n",
        "- Memaksa URL menggunakan protokol https.  \n",
        "\n",
        "**2. normalize_title(t: str)**  \n",
        "\n",
        "Menormalkan judul artikel supaya mudah dibandingkan:  \n",
        "- Mengubah huruf menjadi lowercase.  \n",
        "- Menghapus tanda baca umum.  \n",
        "- Memadatkan spasi berlebih.  \n",
        "\n",
        "**3. dates_close(d1, d2, days=1)**  \n",
        "\n",
        "Mengecek apakah dua tanggal berdekatan, dengan toleransi ±days (default = 1 hari).  \n",
        "Jika salah satu tanggal kosong (None), otomatis dianggap tidak dekat.  \n",
        "\n",
        "**4. deduplicate_rows(rows, fuzzy=True, fuzzy_threshold=0.92)**  \n",
        "\n",
        "Fungsi utama untuk mendeteksi dan menghapus duplikat artikel.  \n",
        "Aturan yang dipakai:  \n",
        "1. Jika canonical URL sama → dianggap duplikat.  \n",
        "2. Jika kombinasi judul normalisasi + tanggal sama → dianggap duplikat.  \n",
        "3. Jika judul sangat mirip (fuzzy match ≥ threshold) dan tanggal berdekatan → dianggap duplikat.  \n",
        "\n",
        "Menghasilkan:  \n",
        "- unique: daftar artikel unik.  \n",
        "- dup_report: laporan artikel duplikat (mana yang disimpan, mana yang dibuang, beserta alasannya).  "
      ],
      "metadata": {
        "id": "twCIsoBLgQd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data Duplicate Detection and Remover\n",
        "\n",
        "def canonicalize_url(u: str) -> str:\n",
        "    \"\"\"\n",
        "    Bikin URL kanonik supaya varian yang sama (utm, source, amp) dianggap satu.\n",
        "    - drop query & fragment\n",
        "    - buang suffix /amp\n",
        "    - rapikan slash\n",
        "    - paksa https\n",
        "    \"\"\"\n",
        "    if not u:\n",
        "        return \"\"\n",
        "    p = urlparse(u)\n",
        "    path = re.sub(r\"/amp/?$\", \"/\", p.path or \"/\")\n",
        "    path = re.sub(r\"//+\", \"/\", path)\n",
        "    if path != \"/\" and path.endswith(\"/\"):\n",
        "        path = path[:-1]\n",
        "    canon = urlunparse((\"https\", p.netloc.lower(), path, \"\", \"\", \"\"))  # tanpa query/fragment\n",
        "    return canon\n",
        "\n",
        "def normalize_title(t: str) -> str:\n",
        "    \"\"\"Lowercase, hilangkan tanda baca umum, padatkan spasi.\"\"\"\n",
        "    if not t:\n",
        "        return \"\"\n",
        "    t = t.lower()\n",
        "    t = re.sub(r\"[^\\w\\s]\", \" \", t, flags=re.UNICODE)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def dates_close(d1, d2, days=1) -> bool:\n",
        "    \"\"\"True jika dua tanggal sama/berdekatan (±days). None dianggap tidak dekat.\"\"\"\n",
        "    if d1 is None or d2 is None:\n",
        "        return False\n",
        "    return abs((d1 - d2).days) <= days\n",
        "\n",
        "def deduplicate_rows(rows, fuzzy=True, fuzzy_threshold=0.92):\n",
        "    \"\"\"\n",
        "    Kembalikan (unique_rows, dup_report)\n",
        "    - unique_rows: list artikel unik\n",
        "    - dup_report : list {keep, drop, reason}\n",
        "    Aturan:\n",
        "      1) Sama canonical URL → duplikat\n",
        "      2) Sama (title_norm, published) → duplikat\n",
        "      3) Fuzzy: mirip judul (>=threshold) & tanggal dekat → duplikat\n",
        "    \"\"\"\n",
        "    unique = []\n",
        "    dup_report = []\n",
        "\n",
        "    url_index = {}                 # canon_url -> idx unique\n",
        "    title_date_index = {}          # (title_norm, published) -> idx unique\n",
        "\n",
        "    for r in rows:\n",
        "        item = dict(r)  # salin agar aman\n",
        "        cu = canonicalize_url(item.get(\"url\", \"\"))\n",
        "        tn = normalize_title(item.get(\"title\", \"\"))\n",
        "        dt = item.get(\"published\")  # diasumsikan sudah tipe date (dari parse_id_date)\n",
        "\n",
        "        # Rule 1: exact canonical URL\n",
        "        if cu and cu in url_index:\n",
        "            dup_report.append({\"reason\": \"same_canonical_url\", \"keep\": unique[url_index[cu]], \"drop\": item})\n",
        "            continue\n",
        "\n",
        "        # Rule 2: same (title_norm, date)\n",
        "        key_td = (tn, dt) if tn and dt else None\n",
        "        if key_td and key_td in title_date_index:\n",
        "            dup_report.append({\"reason\": \"same_title_and_date\", \"keep\": unique[title_date_index[key_td]], \"drop\": item})\n",
        "            continue\n",
        "\n",
        "        # Rule 3: fuzzy title (same/close date)\n",
        "        matched = False\n",
        "        if fuzzy and tn:\n",
        "            for idx, kept in enumerate(unique):\n",
        "                if not dates_close(dt, kept.get(\"published\")):\n",
        "                    continue\n",
        "                kept_tn = normalize_title(kept.get(\"title\", \"\"))\n",
        "                if not kept_tn:\n",
        "                    continue\n",
        "                ratio = difflib.SequenceMatcher(None, tn, kept_tn).ratio()\n",
        "                if ratio >= fuzzy_threshold:\n",
        "                    dup_report.append({\n",
        "                        \"reason\": f\"fuzzy_title_sim>={fuzzy_threshold} (score={ratio:.2f})\",\n",
        "                        \"keep\": kept, \"drop\": item\n",
        "                    })\n",
        "                    matched = True\n",
        "                    break\n",
        "        if matched:\n",
        "            continue\n",
        "\n",
        "        # Keep this item\n",
        "        unique.append(item)\n",
        "        if cu:\n",
        "            url_index[cu] = len(unique) - 1\n",
        "        if key_td:\n",
        "            title_date_index[key_td] = len(unique) - 1\n",
        "\n",
        "    return unique, dup_report"
      ],
      "metadata": {
        "id": "c73rchWeeZnx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proses Deduplikasi Data**\n",
        "\n",
        "Setelah semua artikel dari section berhasil dikumpulkan, langkah selanjutnya adalah menghapus data ganda (duplicate).  \n",
        "Untuk itu kita gunakan fungsi `deduplicate_rows` dengan parameter:  \n",
        "\n",
        "- **rows** → data mentah hasil crawling.  \n",
        "- **fuzzy=True** → aktifkan pencocokan judul mirip (fuzzy matching).  \n",
        "- **fuzzy_threshold=0.92** → ambang kemiripan judul, misalnya ≥92% dianggap sama.  \n",
        "\n",
        "Fungsi ini menghasilkan dua keluaran:  \n",
        "- **unique_rows** → daftar artikel yang benar-benar unik.  \n",
        "- **dup_report** → laporan artikel yang dianggap duplikat beserta alasannya.  \n",
        "\n",
        "Akhirnya, kita cetak ringkasan: berapa jumlah data sebelum deduplikasi, berapa yang unik, dan berapa yang terdeteksi duplikat.  "
      ],
      "metadata": {
        "id": "iDHj4oRHgi_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_rows, dup_report = deduplicate_rows(rows, fuzzy=True, fuzzy_threshold=0.92)\n",
        "print(f\"rows before: {len(rows)} | unique: {len(unique_rows)} | removed dup: {len(dup_report)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqJcjsKjel2S",
        "outputId": "39a740b7-abc4-4cf2-b428-54ab9a699fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rows before: 200 | unique: 199 | removed dup: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Filtering Function**\n",
        "\n",
        "Setelah data terkumpul dan deduplikasi, biasanya kita butuh **menyaring artikel** sesuai kriteria tertentu.  \n",
        "Fungsi filtering ini punya beberapa komponen:\n",
        "\n",
        "1. **within_range(d, start, end)**  \n",
        "   Mengecek apakah tanggal artikel berada dalam rentang tertentu.  \n",
        "   - Jika `d` (tanggal) kosong, keputusan tergantung pada konfigurasi `INCLUDE_WITHOUT_DATE`.  \n",
        "   - Jika ada batas awal (`start`) atau akhir (`end`), artikel dicek apakah sesuai dengan rentang tanggal tersebut.  \n",
        "\n",
        "2. **_join_text_for_match(item, mode)**  \n",
        "   Membuat string gabungan sesuai mode pencarian.  \n",
        "   - `\"title\"` → hanya judul.  \n",
        "   - `\"title+summary\"` → judul + ringkasan.  \n",
        "   - `\"title+text\"` → judul + ringkasan + isi teks.  \n",
        "   Hasil teks bisa dibuat lowercase jika pencarian tidak case-sensitive.  \n",
        "\n",
        "3. **_matches_keywords(text, keywords)**  \n",
        "   Mengecek apakah ada kata kunci yang muncul dalam teks.  \n",
        "   - Jika daftar keyword kosong, otomatis dianggap cocok.  \n",
        "   - Kalau ada keyword, teks diperiksa satu per satu.  "
      ],
      "metadata": {
        "id": "a6QXeNhXpG-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Filtering Function\n",
        "\n",
        "def within_range(d, start, end):\n",
        "    if d is None:\n",
        "        return bool(INCLUDE_WITHOUT_DATE and not start and not end)\n",
        "    s = datetime.strptime(start, \"%Y-%m-%d\").date() if start else None\n",
        "    e = datetime.strptime(end, \"%Y-%m-%d\").date() if end else None\n",
        "    if s and d < s: return False\n",
        "    if e and d > e: return False\n",
        "    return True\n",
        "\n",
        "def _join_text_for_match(item, mode):\n",
        "    parts = []\n",
        "    if mode in (\"title\", \"title+summary\", \"title+text\"):\n",
        "        parts.append(item.get(\"title\",\"\") or \"\")\n",
        "    if mode in (\"title+summary\", \"title+text\"):\n",
        "        parts.append(item.get(\"summary\",\"\") or \"\")\n",
        "    if mode == \"title+text\":\n",
        "        parts.append(item.get(\"text\",\"\") or \"\")\n",
        "    txt = \" \".join(p for p in parts if p)\n",
        "    return txt if CASE_SENSITIVE else txt.lower()\n",
        "\n",
        "def _matches_keywords(text, keywords):\n",
        "    if not keywords:\n",
        "        return True\n",
        "    hay = text if CASE_SENSITIVE else text.lower()\n",
        "    for kw in keywords:\n",
        "        needle = kw if CASE_SENSITIVE else kw.lower()\n",
        "        if needle in hay:\n",
        "            return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "IgFvczpZbzJm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Penyaringan Artikel dengan Keyword dan Rentang Tanggal**\n",
        "\n",
        "Setelah data artikel terkumpul, kita lakukan tahap akhir berupa filtering agar hanya artikel yang relevan yang tersisa.  \n",
        "\n",
        "Langkah filtering meliputi:  \n",
        "1. **Pemeriksaan keyword** → menggunakan `_matches_keywords` untuk cek apakah artikel mengandung salah satu kata kunci yang ditentukan.  \n",
        "2. **Pemeriksaan tanggal tersedia** → menghitung berapa artikel yang memiliki tanggal publikasi (tidak None).  \n",
        "3. **Pemeriksaan rentang tanggal** → menggunakan `within_range` untuk cek apakah artikel terbit dalam rentang `START_DATE` hingga `END_DATE`.  \n",
        "\n",
        "Hanya artikel yang lolos **dua syarat utama** (mengandung keyword dan berada dalam rentang tanggal) yang dimasukkan ke dalam list `filtered`.  \n",
        "\n",
        "Akhirnya, dicetak ringkasan jumlah artikel:  \n",
        "- total artikel hasil crawling,  \n",
        "- berapa yang cocok dengan keyword,  \n",
        "- berapa yang memiliki tanggal,  \n",
        "- berapa yang berada dalam rentang,  \n",
        "- dan total artikel yang tersaring setelah filter.  "
      ],
      "metadata": {
        "id": "5NP5vIB2g4Sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matched_kw = have_date = in_range = 0\n",
        "filtered = []\n",
        "\n",
        "for r in rows:\n",
        "    txt = _join_text_for_match(r, MATCH_IN)\n",
        "    ok_kw   = _matches_keywords(txt, KEYWORDS);               matched_kw += bool(ok_kw)\n",
        "    ok_date = r.get(\"published\") is not None;                 have_date  += bool(ok_date)\n",
        "    ok_rng  = within_range(r.get(\"published\"), START_DATE, END_DATE); in_range += bool(ok_rng)\n",
        "    if ok_kw and ok_rng:\n",
        "        filtered.append(r)\n",
        "\n",
        "print(f\"After crawl: {len(rows)} items | matched_kw: {matched_kw} | have_date: {have_date} | in_range: {in_range} | After filter: {len(filtered)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUcin3gPLwqo",
        "outputId": "7a475f26-1484-421d-cdf8-ae7e4b4a5eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After crawl: 200 items | matched_kw: 5 | have_date: 200 | in_range: 137 | After filter: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dari output tersebut terlihat bahwa kita mendapat 200 berita dengan 4 berita yang sesuai dengan keyword, 200 berita yang tanggal publis nya tersedia, 143 berita yang berada pada tanggal 1 September - 4 September sesuai dengan pengaturan filternya, dan ada 2 berita yang keyword dan tanggalnya sesuai dengan filter."
      ],
      "metadata": {
        "id": "SNfC1q1FuEpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Menampilkan Hasil Artikel yang Sudah Difilter**\n",
        "\n",
        "Tahap terakhir adalah melihat hasil artikel yang sudah lolos filter.  \n",
        "Caranya:  \n",
        "\n",
        "1. **Sortir artikel** → daftar `filtered` diurutkan berdasarkan tanggal publikasi (`published`), dari terbaru ke terlama.  \n",
        "   - Jika ada artikel tanpa tanggal, digunakan nilai minimal `date.min` supaya tetap bisa diurutkan.  \n",
        "\n",
        "2. **Cetak artikel teratas** → ditampilkan maksimal 20 artikel pertama dari hasil sortir.  \n",
        "   Untuk tiap artikel ditampilkan informasi:  \n",
        "   - tanggal publikasi,  \n",
        "   - judul,  \n",
        "   - URL,  \n",
        "   - ringkasan (summary) jika tersedia, dipotong maksimal 180 karakter agar rapi.  "
      ],
      "metadata": {
        "id": "4v1OH4KzgwQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sorted = sorted(filtered, key=lambda x: x.get(\"published\") or date.min, reverse=True)\n",
        "print(\"Scrapping Results:\")\n",
        "for it in filtered_sorted[:20]:\n",
        "    print(\"-\"*130)\n",
        "    print(\"published :\", it.get(\"published\"))\n",
        "    print(\"title     :\", it.get(\"title\"))\n",
        "    print(\"url       :\", it.get(\"url\"))\n",
        "    if it.get(\"summary\"):\n",
        "        s = it[\"summary\"]; print(\"summary   :\", (s[:180] + \"...\") if len(s) > 180 else s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT-_TpNULuo-",
        "outputId": "8a71eb2d-c339-40d5-d48b-ee03074ba01b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scrapping Results:\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "published : 2025-04-09\n",
            "title     : Profil Rusdi Masse, Wakil Ketua Komisi III Pengganti Sahroni, Pernah Jadi Bupati Termuda Nasional 4 September 2025\n",
            "url       : https://nasional.kompas.com/read/2025/09/04/11133361/profil-rusdi-masse-wakil-ketua-komisi-iii-pengganti-sahroni-pernah-jadi\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "published : 2025-04-09\n",
            "title     : Gantikan Ahmad Sahroni, Rusdi Masse Mappassesu Dilantik Jadi Wakil Ketua Komisi III Nasional 4 September 2025\n",
            "url       : https://nasional.kompas.com/read/2025/09/04/10055731/gantikan-ahmad-sahroni-rusdi-masse-mappassesu-dilantik-jadi-wakil-ketua\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Selanjutnya kita bisa menyimpannya ke dalam csv dan siap untuk diolah lebih lanjut."
      ],
      "metadata": {
        "id": "LhSUILNo4lMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows_csv = [{\n",
        "    \"published\": (it.get(\"published\").isoformat()\n",
        "                  if hasattr(it.get(\"published\"), \"isoformat\") else it.get(\"published\")),\n",
        "    \"title\": it.get(\"title\"),\n",
        "    \"url\": it.get(\"url\")\n",
        "} for it in filtered_sorted]\n",
        "\n",
        "df = pd.DataFrame(rows_csv)\n",
        "\n",
        "# df.to_csv(\"/content/kompas_filtered.csv\", index=False, encoding=\"utf-8\")\n",
        "# print(\"CSV disimpan ke /content/kompas_filtered.csv, rows:\", len(df))"
      ],
      "metadata": {
        "id": "QMN7XPlL1kJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "JtJymZF62bKK",
        "outputId": "a0969c22-80c3-4004-a19f-1a01d808749e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    published                                              title  \\\n",
              "0  2025-04-09  Profil Rusdi Masse, Wakil Ketua Komisi III Pen...   \n",
              "1  2025-04-09  Gantikan Ahmad Sahroni, Rusdi Masse Mappassesu...   \n",
              "\n",
              "                                                 url  \n",
              "0  https://nasional.kompas.com/read/2025/09/04/11...  \n",
              "1  https://nasional.kompas.com/read/2025/09/04/10...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1c51d9b-8c52-43f0-925d-2972dd97a05e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>published</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-04-09</td>\n",
              "      <td>Profil Rusdi Masse, Wakil Ketua Komisi III Pen...</td>\n",
              "      <td>https://nasional.kompas.com/read/2025/09/04/11...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-04-09</td>\n",
              "      <td>Gantikan Ahmad Sahroni, Rusdi Masse Mappassesu...</td>\n",
              "      <td>https://nasional.kompas.com/read/2025/09/04/10...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1c51d9b-8c52-43f0-925d-2972dd97a05e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e1c51d9b-8c52-43f0-925d-2972dd97a05e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e1c51d9b-8c52-43f0-925d-2972dd97a05e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-72cccdb5-0b2f-452b-a190-e2a251d00f51\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-72cccdb5-0b2f-452b-a190-e2a251d00f51')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-72cccdb5-0b2f-452b-a190-e2a251d00f51 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2025-04-09\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Gantikan Ahmad Sahroni, Rusdi Masse Mappassesu Dilantik Jadi Wakil Ketua Komisi III Nasional 4 September 2025\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"https://nasional.kompas.com/read/2025/09/04/10055731/gantikan-ahmad-sahroni-rusdi-masse-mappassesu-dilantik-jadi-wakil-ketua\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Note:**\n",
        "Dikarenakan pada laman artikel https://nasional.kompas.com/ maksimal hanya menampilkan 10 page, jadi jika ada berita dengan tanggal lebih lama dibanding berita terakhir di halaman ke-10 maka berita tersebut tidak dapat ditampilkan."
      ],
      "metadata": {
        "id": "RcN4kDYcr1Pk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sekian, arigatou.."
      ],
      "metadata": {
        "id": "1_irKwRfEk-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"align-left\">\n",
        "  <img src=\"https://media1.tenor.com/m/u-0BLG9HxAQAAAAC/mambo-matikanetannhauser.gif\" width=\"320\" alt=\"Mambo\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "PK_BnaeDEYhd"
      }
    }
  ]
}